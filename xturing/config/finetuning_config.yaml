defaults:
  learning_rate: 1e-5
  gradient_accumulation_steps: 1
  batch_size: 1
  weight_decay: 0.00
  warmup_steps: 50
  eval_steps: 5000
  save_steps: 5000
  max_length: 512
  num_train_epochs: 1
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 4
  optimizer_name: adamw
  output_dir: saved_model

llama_lora_int4:
  learning_rate: 1e-4
  weight_decay: 0.01
  num_train_epochs: 1
  batch_size: 8
  max_length: 512
